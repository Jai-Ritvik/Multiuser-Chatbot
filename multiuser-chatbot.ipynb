{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57745,"status":"ok","timestamp":1750828133799,"user":{"displayName":"Quick Loss","userId":"09856312205758067363"},"user_tz":-330},"id":"tDSoI0NTE63W","outputId":"6a9ebf68-d5ea-45a9-ad6c-daf5ca8a6bc2"},"outputs":[],"source":["# System setup: APT packages, Ollama, CUDA drivers\n","!sudo apt-get update -y\n","!sudo apt-get install -y curl pciutils lsb-release\n","!curl -fsSL https://ollama.com/install.sh | sh\n","\n","# Check GPU\n","!nvidia-smi || true"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125799,"status":"ok","timestamp":1750828259600,"user":{"displayName":"Quick Loss","userId":"09856312205758067363"},"user_tz":-330},"id":"F76vLbsWE9R3","outputId":"a8271d83-74b1-403a-da19-d934210dd959"},"outputs":[],"source":["#  Python dependencies\n","%%bash\n","pip install -q \\\n","  langchain-core \\\n","  langchain-community \\\n","  langchain-chroma \\\n","  langchain-ollama \\\n","  sentence-transformers \\\n","  chromadb \\\n","  gradio \\\n","  pymupdf \\\n","  \"unstructured[docx]\" \\\n","  tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":931,"status":"ok","timestamp":1750828405672,"user":{"displayName":"Quick Loss","userId":"09856312205758067363"},"user_tz":-330},"id":"fnalamCdE-c1","outputId":"c3f62782-b637-49ed-99c7-61f30719f5c6"},"outputs":[],"source":["#  Pull the LLM & start Ollama server\n","import subprocess, threading, requests, time, os\n","\n","LLM_MODEL      = \"mistral:7b\"\n","EMBED_MODEL    = \"nomic-embed-text\"\n","\n","def _serve():\n","    subprocess.Popen([\"ollama\", \"serve\"],\n","                     stdout=subprocess.DEVNULL,\n","                     stderr=subprocess.DEVNULL)\n","\n","threading.Thread(target=_serve, daemon=True).start()\n","\n","# Wait until Ollama REST endpoint is up\n","for _ in range(20):\n","    try:\n","        if requests.get(\"http://localhost:11434\").ok:\n","            break\n","    except:\n","        time.sleep(1)\n","else:\n","    raise RuntimeError(\"‚ùå Ollama failed to start.\")\n","\n","!ollama pull {LLM_MODEL}\n","!ollama pull {EMBED_MODEL}\n","\n","print(\"‚úÖ Ollama ready with:\", LLM_MODEL, \"and\", EMBED_MODEL)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2062,"status":"ok","timestamp":1750828409783,"user":{"displayName":"Quick Loss","userId":"09856312205758067363"},"user_tz":-330},"id":"BPJy55tPE_mW"},"outputs":[],"source":["# Paths & global objects\n","import warnings, uuid, shutil, tempfile, os\n","from pathlib import Path\n","from typing import List\n","\n","# LangChain imports\n","from langchain_community.document_loaders import (\n","    PyPDFLoader, CSVLoader, TextLoader,\n","    UnstructuredMarkdownLoader, UnstructuredWordDocumentLoader\n",")\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_chroma import Chroma\n","from langchain_community.llms import Ollama\n","from langchain_ollama.embeddings import OllamaEmbeddings\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import RetrievalQA\n","\n","# Embedder & splitter\n","EMBEDDER = OllamaEmbeddings(model=\"nomic-embed-text\")\n","SPLITTER = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n","\n","# Safe prompt: model must stick to context\n","SAFE_PROMPT = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=(\n","        \"Use ONLY the context below to answer the question. \"\n","        \"If the answer is not in the context, reply exactly: \"\n","        \"'I don't know based on the provided documents.'\\n\\n\"\n","        \"Context:\\n{context}\\n\\n\"\n","        \"Question: {question}\\nAnswer:\"\n","    )\n",")\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1750828410113,"user":{"displayName":"Quick Loss","userId":"09856312205758067363"},"user_tz":-330},"id":"SxprdSEFFBiW"},"outputs":[],"source":["#  Utility functions (read docs, build index, etc.)\n","def read_files_local(paths: List[Path]):\n","    \"\"\"Return LangChain Docs from various file types.\"\"\"\n","    docs = []\n","    for p in paths:\n","        try:\n","            if p.suffix == \".pdf\":\n","                loader = PyPDFLoader(str(p))\n","            elif p.suffix == \".csv\":\n","                loader = CSVLoader(str(p))\n","            elif p.suffix == \".txt\":\n","                loader = TextLoader(str(p), encoding=\"utf-8\")\n","            elif p.suffix == \".md\":\n","                loader = UnstructuredMarkdownLoader(str(p))\n","            elif p.suffix == \".docx\":\n","                loader = UnstructuredWordDocumentLoader(str(p))\n","            else:\n","                print(f\"[!] Skipping unsupported: {p.name}\")\n","                continue\n","            for d in loader.load():\n","                d.metadata[\"source\"] = p.name\n","                docs.append(d)\n","        except Exception as e:\n","            print(f\"[!] Failed on {p.name}: {e}\")\n","    return docs\n","\n","def build_vectorstore(docs, sid):\n","    chunks = SPLITTER.split_documents(docs)\n","    return Chroma.from_documents(\n","        chunks, embedding=EMBEDDER,\n","        collection_name=f\"session-{sid}\", persist_directory=None\n","    )\n","\n","def make_qa_chain(vstore):\n","    retriever = vstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n","    llm = Ollama(model=LLM_MODEL)\n","    return RetrievalQA.from_chain_type(\n","        llm=llm,\n","        retriever=retriever,\n","        chain_type=\"stuff\",\n","        return_source_documents=True,\n","        chain_type_kwargs={\"prompt\": SAFE_PROMPT}\n","    )"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3080,"status":"ok","timestamp":1750828415904,"user":{"displayName":"Quick Loss","userId":"09856312205758067363"},"user_tz":-330},"id":"RjwFWGxfFEue"},"outputs":[],"source":["#  Create global QA chain (with safe prompt and source viewing)\n","import gradio as gr\n","\n","def ingest_fn(state, files):\n","    if not files:\n","        return state, \"‚ö†Ô∏è Please upload at least one file.\"\n","\n","    sid = state.setdefault(\"id\", str(uuid.uuid4()))\n","    tmp_dir = Path(tempfile.mkdtemp(prefix=f\"u_{sid}_\"))\n","    local_paths = []\n","\n","    # Copy uploads for stable file paths\n","    for f in files:\n","        target = tmp_dir / Path(f.name).name\n","        shutil.copy(f.name, target) if hasattr(f, \"name\") else None\n","        local_paths.append(target)\n","\n","    docs = read_files_local(local_paths)\n","    if not docs:\n","        return state, \"‚ö†Ô∏è Could not parse the uploaded files.\"\n","\n","    vstore = build_vectorstore(docs, sid)\n","    state[\"qa_chain\"] = make_qa_chain(vstore)\n","    state[\"history\"] = []\n","    return state, f\"‚úÖ Indexed {len(docs)} document(s). Ask away!\"\n","\n","def chat_fn(state, user_msg):\n","    user_msg = (user_msg or \"\").strip()\n","    hist = state.get(\"history\", [])\n","\n","    if not user_msg:\n","        return state, hist\n","\n","    if \"qa_chain\" not in state:\n","        hist.append((user_msg, \"‚ö†Ô∏è Upload docs first.\"))\n","        return state, hist\n","\n","    hist.append((user_msg, \"‚è≥ ‚Ä¶thinking‚Ä¶\"))\n","    yield state, hist\n","\n","    try:\n","        res = state[\"qa_chain\"].invoke({\"query\": user_msg})\n","        answer = res[\"result\"]\n","    except Exception as e:\n","        answer = f\"‚ö†Ô∏è {type(e).__name__}: {e}\"\n","\n","    hist[-1] = (user_msg, answer)\n","    yield state, hist\n","\n","def clear_chat(state):\n","    state[\"history\"] = []\n","    return state, []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"elapsed":892619,"status":"ok","timestamp":1750829320157,"user":{"displayName":"Quick Loss","userId":"09856312205758067363"},"user_tz":-330},"id":"Q1oyqfkuFGDe","outputId":"a62643a2-a3df-40cf-b358-f5edfb83e243"},"outputs":[],"source":["#  Multi‚Äësession / multi‚ÄëKB Gradio app\n","with gr.Blocks(title=\"RAG Chatbot (multi‚Äëuser)\") as demo:\n","    gr.Markdown(\"## üìÅ Upload docs & ask questions ‚Äî isolated per session\")\n","\n","    with gr.Row():\n","        uploads = gr.Files(file_count=\"multiple\", label=\"Upload files\")\n","        idx_btn = gr.Button(\"Index documents\", variant=\"primary\")\n","    status = gr.Markdown()\n","\n","    chatbot   = gr.Chatbot(label=\"Chatbot\", height=430)\n","    user_box  = gr.Textbox(label=\"Your question\", placeholder=\"Type and hit Enter‚Ä¶\")\n","    clear_btn = gr.Button(\"Clear chat\")\n","\n","    session_state = gr.State({})\n","\n","    idx_btn.click(\n","        ingest_fn, inputs=[session_state, uploads], outputs=[session_state, status]\n","    )\n","    user_box.submit(\n","        chat_fn, inputs=[session_state, user_box], outputs=[session_state, chatbot]\n","    )\n","    clear_btn.click(\n","        clear_chat, inputs=session_state, outputs=[session_state, chatbot]\n","    )\n","\n","demo.queue().launch(share=True, debug=True)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN3ndR42qLUt1M7ElBhGWHT","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
